\section{Related Work}\label{sec:relwork}

Compilation for distribution memory machines has two main steps: loop optimizations and message passing code generation. First, the compiler performs loop transformations and optimizations to uncover parallelism, improve the granularity of parallelism, and improve cache performance. These transformations include loop peeling, loop reversal, and loop interchange. Chapel is an explicitly parallel language, so uncovering parallelism is not needed. Other loop optimizations to improve the granularity of parallelism and improve cache performance are orthogonal to this paper. The second step is message passing code generation, which includes message aggregation.

Message passing code generation in the traditional model is exceedingly complex, and practical robust implementations are hard to find. These methods \cite{goumas2006message, xue1997communication, callahan1988compiling, ramanujam1991compile} require footprint calculations for each tile. A footprint is the span of data elements accessed by all iterations of a single tile. It is common for a tile's footprint to span across multiple locales, requiring communication between locales. Footprint calculations are modeled by matrices and need to be intersected with the data distribution in order to determine the locality of data elements. Message aggregation can only be done once the compiler determines which data elements of a tile's footprint are remote. These footprint calculations quickly become more complex as the number of locales scales. Our method does not require any footprint calculations, thereby simplifying code generation. 

The polyhedral method is another branch of compiler optimization that seeks to speed up parallel programs on distributed memory architectures \cite{chavarria2005effective, germain1995automatic, Gupta91automaticdata, gupta1996compiling, iancu2008performance, wei1998compiling}. The primary purpose of the polyhedral method is uncovering parallelism and loop optimization, not code generation. Its strength is that it can find sequences of transformations in one step, without searching the entire space of transformations. However, the method at its core does not compute information for message passing code generation. Message passing code generation does not fit the polyhedral model, so ad-hoc methods for code generation have been devised to work on the output of the polyhedral model. However they are no better than corresponding methods in the traditional model, and suffer from many of the same difficulties.

Similar work to take advantage of communication aggregation on distributed arrays has already been done in Chapel. Like distributed parallel loops in Chapel, whole array assignment suffered from locality checks for every array element, even when the locality of certain elements is known in advance. In \cite{sanz2012global}, aggregation was applied to improve the communication performance of whole array assignments for Chapel's Block and Cyclic distributions. Our work goes beyond array assignments and applies aggregation to affine array accesses within parallel loops for Chapel's Cyclic and Block Cyclic distributions. One of the contribution's of \cite{sanz2012global} included two new bulk communication primitives for Chapel developers as library calls, \texttt{chpl\_comm\_gets} and \texttt{chpl\_comm\_puts}. They both rely on the GASNet networking layer, a portion of the Chapel runtime.  Our optimization uses these new communication primitives in our implementation directly to perform bulk remote data transfer between locales.

Extensive work has been done with the UPC compiler (another PGAS language) by \cite{chen2005communication} to improve on its communication performance. This method targets fine-grained communication and uses techniques such as redundancy elimination, split-phase communication, and communication coalescing (similar to message aggregation) to reduce overall communication. However, it is not clear whether this method can be used to improve communication performance across distributed parallel loops. There is no locality analysis that statically determines whether an array access is shared or remote. Our method, modulo unrolling, can determine which accesses are local purely on the affine array access and data distribution. Another major limitation to this work's aggregation scheme is that only contiguous data can be sent in bulk. To aggregate data across an entire loop in a single message, it must be possible to aggregate data elements that are far apart in memory, separated by a fixed stride. Our method handles this by using the strided get and put calls (\texttt{chpl\_comm\_gets} and \texttt{chpl\_comm\_puts}) from \cite{sanz2012global}, described earlier. 

\begin{comment}
\cite{callahan1988compiling}
\cite{chamberlain1998zpl}
\cite{chamberlain1997factor}
\cite{chavarria2005effective}
\cite{davidson1995improving}
\cite{Dion96compilingaffine}
\cite{germain1995automatic}
\cite{Gupta91automaticdata}
\cite{gupta1996compiling}
\cite{huang1994speculative}
\cite{iancu2008performance}
\cite{li1991data}
\cite{pouchet2011loop}
\cite{ramanujam1991compile}
\cite{shih2000efficient}
\cite{trifunovic2010graphite}
\cite{wei1998compiling}
\cite{chamberlain2011user}
\cite{bonachea2007proposal}
\cite{sanz2012global}
\end{comment}