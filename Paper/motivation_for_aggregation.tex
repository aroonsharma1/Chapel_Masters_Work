\section{Motivation for Message Aggregation}\label{sec:motivation_for_aggregation} 

In Chapel, a program's data access patterns and the programmer's choice of data distribution greatly influence the program's runtime and communication behavior. There are some situations where programs exhibit predictable patterns of communication that the compiler can detect. In doing so, the compiler can aggregate remote data elements coming from one locale into one local buffer via a single message and then access this local buffer on subsequent iterations of the loop. 

For example, consider Chapel code for the Jacobi computation shown in Figure \ref{jacobi_code}, a common stencil operation that computes elements of a two dimensional array as an average of that element's four adjacent neighbors. On each iteration of the loop, five array elements are accessed in an affine manner: the current array element $A_{new}[i, j]$ and its four adjacent neighbors $A[i+1, j]$, $A[i-1, j]$, $A[i, j+1]$, and $A[i, j-1]$. Naturally, the computation will take place on the locale of $A_{new}[i, j]$, the element being written to. If arrays $A$ and $A_{new}$ are distributed with a Cyclic distribution as shown in Figure \ref{cyc_dist}, then it is guaranteed that $A[i+1, j]$, $A[i-1, j]$, $A[i, j+1]$, and $A[i, j-1]$ will not reside on the same locale as $A_{new}[i, j]$ \textbf{for all iterations of the loop}. These remote elements are transferred over to $A_{new}[i, j]$'s locale in four individual messages during every loop iteration. For large data sets, transferring four elements individually per loop iteration drastically slows down the program because the message overhead is incurred more than once. 

Since the data is distributed using a Cyclic distribution, we notice that the data is accessed in the same way every cycle. For example, on iteration $(2, 2)$, $A_{new}[2, 2]$ resides on locale 3, $A[2, 1]$ and $A[2, 3]$ reside on locale 1, and $A[1, 2]$ and $A[3, 2]$ reside on locale 2. If we look at iteration $(4, 2)$ which is an iteration in the next cycle, we see that $A_{new}[4, 2]$ also resides on locale 3, $A[4, 1]$ and $A[4, 3]$ reside on locale 1, and $A[3, 2]$ and $A[5, 2]$ reside on locale 2. We can therefore bring in all remote data elements accessed by iterations where $A_{new}[i, j]$ resides on locale 3 to locale 3 before the loop executes and write them back to locales 1 and 2 after the loop finishes. 

If we focus on locale 3, there will be four buffers containing remote data elements after aggregation has occurred, one for each affine array access in the loop in Figure \ref{jacobi_code}. The first buffer is known conceptually as $buf$\_$north$ and will contain the array slice $A[2..7$ $by$ $2, 1..6$ $by$ $2]$. The elements in this buffer correspond to all elements produced by the $A[i, j-1]$ array access over all iterations of the loop in Figure \ref{jacobi_code} where $A_{new}[i, j]$ also resides on locale 3. Similarly, there are buffers $buf$\_$south$, $buf$\_$east$, and $buf$\_$west$ that contain a similar description of elements for array accesses $A[i, j+1]$, $A[i+1, j]$, and $A[i-1, j]$ respectively. Now that a copy of all remote data elements reside on the locale that they are used from, the affine array accesses other than $A_{new}[i, j]$ can be replaced with accesses to the local buffers. After the loop has finished, any buffer elements that have been written to are communicated back to their respective remote locales in their own aggregate messages. This optimization can also be applied to the Block Cyclic distribution, as the data access pattern is the same for elements in the same position within a block. 

If arrays $A$ and $A_{new}$ are instead distributed using Chapel's Block or Block Cyclic distributions as shown in Figure \ref{block_dist} and Figure \ref{block_cyc_dist} respectively, the program will only perform remote data accesses on iterations of the loop where element $A_{new}[i, j]$ is on the boundary of a block. As the blocksize increases, the number of remote data accesses for the Jacobi computation decreases. For the Jacobi computation, it is clear that distributing the data using Chapel's Block distribution is the best choice in terms of communication. Executing the program using a Block distribution will result in fewer remote data accesses than when using a Block Cyclic distribution. Similarly, executing the program using a Block Cyclic distribution will result in fewer remote data accesses than when using a Cyclic distribution. 

It is important to note that the Block distribution is not the best choice for all programs using affine array accesses. Programs with strided access patterns that use a Block distribution will have poor communication performance because accessed array elements are more likely to reside outside of a block boundary. For these types of programs, a Cyclic or Block Cyclic distribution will perform better. 
