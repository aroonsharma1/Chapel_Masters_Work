\section{Motivation for Message Aggregation}\label{sec:motivation_for_aggregation} 

In Chapel, a program's data access patterns and the programmer's choice of data distribution greatly influence the program's runtime and communication behavior. For example, consider Chapel code for the Jacobi computation shown in Figure \ref{jacobi_code}, a common stencil operation that computes elements of a two dimensional array as an average of that element's four adjacent neighbors. On each iteration of the loop, five array elements are accessed in an affine manner: the current array element $A_{new}[i, j]$ and its four adjacent neighbors $A[i+1, j]$, $A[i-1, j]$, $A[i, j+1]$, and $A[i, j-1]$. Naturally, the computation will take place on the locale of $A_{new}[i, j]$, the element being written to. If arrays $A$ and $A_{new}$ are distributed with a Cyclic distribution as shown in Figure \ref{cyc_dist}, then it is guaranteed that $A[i+1, j]$, $A[i-1, j]$, $A[i, j+1]$, and $A[i, j-1]$ will not reside on the same locale as $A_{new}[i, j]$ \textbf{for all iterations of the loop}. Therefore, these remote elements are transferred over to $A_{new}[i, j]$'s locale in four individual messages every iteration. For large data sets, transferring four elements individually per loop iteration drastically slows down the program because the message overhead is incurred many times. 

Since the data is distributed using a Cyclic distribution, we notice that the data is accessed in the same way every cycle. For example, on iteration $(2, 2)$, $A_{new}[2, 2]$ resides on locale 3, $A[2, 1]$ and $A[2, 3]$ reside on locale 1, and $A[1, 2]$ and $A[3, 2]$ reside on locale 2. If we look at iteration $(4, 2)$ which is an iteration in the next cycle, we see that $A_{new}[4, 2]$ also resides on locale 3, $A[4, 1]$ and $A[4, 3]$ reside on locale 1, and $A[3, 2]$ and $A[5, 2]$ reside on locale 2. We can therefore bring in all remote data elements accessed by locale 3 to locale 3 before the loop executes and write them back to locales 1 and 2 after the loop finishes. This optimization can also be applied to the Block Cyclic distribution, as the data access pattern is the same for elements in the same position within a block. 

If arrays $A$ and $A_{new}$ are instead distributed using Chapel's Block or Block Cyclic distributions as shown in Figure \ref{block_dist} and Figure \ref{block_cyc_dist} respectively, the program will only perform remote data accesses on iterations of the loop where element $A_{new}[i, j]$ is on the boundary of a block. As the blocksize increases, the number of remote data accesses for the Jacobi computation decreases. For Jacobi, it is clear that distributing the data using Chapel's Block distribution is the best choice in terms of communication. Executing the program using a Block distribution will result in fewer remote data accesses than when using a Block Cyclic distribution. Similarly, executing the program using a Block Cyclic distribution will result in fewer remote data accesses than when using a Cyclic distribution. 

It is important to note that the Block distribution is not the best choice for all programs using affine array accesses. Programs with strided access patterns that use a Block distribution will have poor communication performance because accessed array elements are more likely to reside outside of a block boundary. For these types of programs, a Cyclic or Block Cyclic distribution will perform better. 
